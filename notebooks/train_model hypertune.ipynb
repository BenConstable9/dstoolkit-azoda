{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Detection Model - Hyper Parameter Tuning\n",
    "\n",
    "This notebook builds on the basic train_model.py but show how you can setup model param parsing, set an early stopping policy and execute this using AML hypertune.\n",
    "\n",
    "Before executing the code please ensure you have followed the setup in the wiki and ensured you have the following:\n",
    "- AML Workspace\n",
    "- Blob or fileshare containing images and label version files\n",
    "- Pretrained model from TF model zoo in the same storage\n",
    "- Built docker image registered to ACR\n",
    "- Local conda environment with the requirements and packages installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal, BanditPolicy\n",
    "from azureml.train.hyperdrive import RandomParameterSampling, choice\n",
    "\n",
    "from azure_utils.azure import load_config\n",
    "from azure_utils.experiment import AMLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Run Parameters\n",
    "\n",
    "Below sets the run paramters including the dockerfile path, base model and datasets. Note that if you datasets are in the same date naming convention you can use the latest keyword to automatically retrieve the latest version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run params    \n",
    "env_config_file = \"dev_config.json\"\n",
    "\n",
    "# Train with TensorFlow 1 use docker built from tf_1 - \"csaddevamlacr.azurecr.io/tfod_tf1:test\"\n",
    "# Train with TensorFlow 2 use docker built from tf_2 - \"csaddevamlacr.azurecr.io/tfod_tf2:test\"\n",
    "docker_image = \"csaddevamlacr.azurecr.io/tfod_tf2:test\"\n",
    "\n",
    "# Train with TF1 use - \"train_hypertune.py\"\n",
    "# Train with TF2 use - \"train_tf2_hypertune.py\"\n",
    "training_script = \"train_tf2_hypertune.py\"\n",
    "\n",
    "# Description\n",
    "desc = \"Testing TF2 with hypertune\"\n",
    "# Experiment name\n",
    "experiment_name = \"pothole\"\n",
    "    \n",
    "# Training and test data selction\n",
    "store_name = \"test_data\"\n",
    "img_type = \"pothole\"\n",
    "train_csv = \"latest\"\n",
    "test_csv = \"latest\"\n",
    "\n",
    "# Base model Selection\n",
    "# Train with TF1 use - \"faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28\"\n",
    "# Train with TF2 use - \"faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8\"\n",
    "base_model = \"faster_rcnn_inception_resnet_v2_1024x1024_coco17_tpu-8\"\n",
    "\n",
    "# Model Params\n",
    "steps = 1000\n",
    "eval_conf = 0.5\n",
    "\n",
    "# model hyparams - model arch specific\n",
    "# Set base parameter here and in Step 7 we set the HPT parameters\n",
    "fs_nms_iou = 0.5 # first stage NMS IOU threshold\n",
    "fs_nms_score = 0.0 # first stage NMS score threshold\n",
    "fs_max_prop = 200 # first stage max proposals\n",
    "fs_loc_loss = 2.0 # first stage localisation loss weight\n",
    "fs_obj_loss = 1.0 # first stage objective loss weight\n",
    "\n",
    "# Compute Params\n",
    "cluster_name = \"train-dev-2\"\n",
    "vm_type = \"STANDARD_NC6\"\n",
    "nodes = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialise Experiment Class\n",
    "\n",
    "Below creates and instance of the experiment class int he Azure utils package, it takes a config file to point to a speciifc AML workspace and the experiment name for usecase grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_exp = AMLExperiment(experiment_name, config_file=env_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Set AML Datastore reference\n",
    "\n",
    "This package makes use of the old approach of mounting the entire datastore in order to access images, dataset files and base models. Below sets up the defined datastore to mount on execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_exp.set_datastore(store_name)\n",
    "aml_exp.set_data_reference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create/Set Compute\n",
    "\n",
    "Below checks if a compute with the provided name exists in the AML workspace and if not creates based on the spec. It takes arguments for node count and vm type with the base compute set to \"STANDARD_NC\" in order to provide GPU support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_exp.set_compute(cluster_name, vm_type=vm_type, node_count=nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Set script params and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_params = [\n",
    "    '--desc', desc,\n",
    "    '--data_dir', str(aml_exp.data_ref),\n",
    "    '--image_type', img_type,\n",
    "    '--train_csv', train_csv,\n",
    "    '--test_csv', test_csv,\n",
    "    '--base_model', base_model,\n",
    "    '--steps', steps,\n",
    "    '--fs_nms_iou', fs_nms_iou,\n",
    "    '--fs_nms_score', fs_nms_score,\n",
    "    '--fs_max_prop', fs_max_prop,\n",
    "    '--fs_loc_loss', fs_loc_loss,\n",
    "    '--fs_obj_loss', fs_obj_loss]\n",
    "\n",
    "# Copy train file to /notebooks\n",
    "shutil.copy(os.path.join(r'..\\src\\training\\scripts', training_script), os.path.join('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Run Config\n",
    "\n",
    "Create run config brings together the compute, script , params and docker image to form a script run configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = os.path.join('.')\n",
    "aml_exp.set_runconfig(scripts,\n",
    "                      training_script,\n",
    "                      script_params,\n",
    "                      docker_image=docker_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Define Hypertune policy and parameter sweeps\n",
    "\n",
    "Below we set a sweep for two of our hyperparams and set an early stop policy for each 100 logging steps. \n",
    "\n",
    "This means that each node with is selection of params that is not within the slackfactor of the current best loss will be stopped early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = RandomParameterSampling({\n",
    "    '--fs_nms_iou': choice(0.5, 0.6, 0.7),\n",
    "    '--fs_max_prop': choice(100, 200, 300)})\n",
    "policy = BanditPolicy(evaluation_interval=100, slack_factor=0.25)\n",
    "metric_name = 'Train - Total Training Loss',\n",
    "metric_goal = PrimaryMetricGoal.MINIMIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Submit\n",
    "\n",
    "Finally execute the configuration defined above to AML. The execution can then be monitored from the AML studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml_exp.submit_hypertune(ps, policy, metric_name, metric_goal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "92559d5c42cc7a16912616893a50c545b5466848a23f39dff16cc750d6bad3ea"
  },
  "kernelspec": {
   "display_name": "dstkpy37",
   "language": "python",
   "name": "dstkpy37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
